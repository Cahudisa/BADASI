{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1c8a75",
   "metadata": {},
   "source": [
    "Paso 1: Descarga de mseed por estacion que contienen solo 1 sismo y sus canales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "csv_file = '../data/Catalogo_Ultimo.csv'\n",
    "\n",
    "# Leer CSV y limpiar encabezados\n",
    "df = pd.read_csv(csv_file, skipinitialspace=True)\n",
    "df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "\n",
    "print(\"Columnas encontradas:\", df.columns.tolist())\n",
    "\n",
    "# Sin el límite de descargas\n",
    "count = 0\n",
    "\n",
    "download_folder = 'A_REDNE_mseed_downloads'\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Estaciones de interés: UIS01, UIS02, ..., UIS11\n",
    "stations = [f\"UIS{str(i).zfill(2)}\" for i in range(1, 12)]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # Ajusta el nombre exacto de la columna después de limpiar\n",
    "        start_time_str = row['Fecha-Hora  (UTC)']\n",
    "\n",
    "        # Restar minutos para start_time y sumar minutos para end_time\n",
    "        base_time = datetime.strptime(start_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "        start_time = base_time - timedelta(minutes=0.1)\n",
    "        end_time = base_time + timedelta(minutes=1.9)\n",
    "\n",
    "        start_fmt = start_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "        end_fmt = end_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "        base_url = \"https://osso.univalle.edu.co/fdsnws/dataselect/1/query\"\n",
    "\n",
    "        # Iterar sobre cada estación\n",
    "        for station in stations:\n",
    "            url = (f\"{base_url}?starttime={start_fmt}&endtime={end_fmt}\"\n",
    "                   f\"&network=UX&station={station}&channel=***&nodata=404\")\n",
    "\n",
    "            #print(f\"\\n[{count+1}] Intentando descargar archivo para estación {station} y rango: {start_fmt} - {end_fmt}\")\n",
    "            #print(f\"URL: {url}\")\n",
    "\n",
    "            # Desactivar la verificación de certificado SSL\n",
    "            response = requests.get(url, timeout=15, verify=False)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                #print(f\"    Error HTTP {response.status_code} - saltando estación {station}...\")\n",
    "                continue\n",
    "\n",
    "            if b'no waveform data found' in response.content.lower():\n",
    "                #print(f\"    No hay datos para la estación {station} en este rango, saltando...\")\n",
    "                continue\n",
    "\n",
    "            filename = f\"{station}_{start_fmt.replace(':', '-')}_{end_fmt.replace(':', '-')}.mseed\"\n",
    "            filepath = os.path.join(download_folder, filename)\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            print(f\"    Archivo guardado en: {filepath}\")\n",
    "            count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(f\"    Error inesperado en fila {idx}: {e}, saltando...\")\n",
    "        continue\n",
    "\n",
    "print(f\"Descargas completadas: {count} archivos descargados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71ac83",
   "metadata": {},
   "source": [
    "Paso 2: La estacion UIS05, se separa en 2, una real con sus canales de aceleracion, y otra estacion virtual con canales de velocidad, como se ve la estacion UIS05 tiene 4 canales EHZ, ENZ, ENE y ENN, 3 de aceleracion (ENZ, ENE y ENN) y  1 de velocidad (EHZ), lo que quiero es que  para la estacion virtual se me duplique el canal (EHZ) en 2 canales mas y que se tranforemen en (EHE) ESTE y (EHN) NORTE y se llame UIS05V, y la otra estacion UIS05 queda solo con los 3 canales de aceleracion (ENZ, ENE y ENN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import read\n",
    "import shutil\n",
    "\n",
    "# Definir la carpeta donde están los archivos mseed descargados\n",
    "download_folder = 'A_REDNE_mseed_downloads'\n",
    "output_folder = 'B_REDNE_mseed_UIS05virtual'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Función para procesar cada archivo mseed y separarlo\n",
    "def process_mseed(file_path):\n",
    "    # Leer el archivo mseed\n",
    "    st = read(file_path)\n",
    "\n",
    "    # Comprobar si la estación en el archivo es UIS05\n",
    "    is_uis05 = any('UIS05' in tr.stats.station for tr in st)\n",
    "\n",
    "    # Si no es de la estación UIS05, copiar el archivo tal cual está\n",
    "    if not is_uis05:\n",
    "        shutil.copy(file_path, output_folder)\n",
    "        print(f\"Archivo {file_path} copiado sin cambios.\")\n",
    "        return\n",
    "\n",
    "    # Crear dos streams separados para los canales de aceleración y velocidad\n",
    "    acceleration_stream = []\n",
    "    velocity_stream = []\n",
    "\n",
    "    # Filtrar los canales de la estación UIS05\n",
    "    for tr in st:\n",
    "        if 'UIS05' in tr.stats.station:\n",
    "            if tr.stats.channel in ['ENZ', 'ENE', 'ENN']:\n",
    "                # Agregar canales de aceleración al stream de aceleración\n",
    "                acceleration_stream.append(tr)\n",
    "            elif tr.stats.channel == 'EHZ':\n",
    "                # Agregar canal de velocidad EHZ al stream de velocidad\n",
    "                velocity_stream.append(tr)\n",
    "                \n",
    "                # Crear los canales EHE y EHN con los mismos datos que EHZ\n",
    "                tr_ehe = tr.copy()\n",
    "                tr_ehe.stats.channel = 'EHE'\n",
    "                tr_ehn = tr.copy()\n",
    "                tr_ehn.stats.channel = 'EHN'\n",
    "                \n",
    "                # Agregar los nuevos canales al stream de velocidad\n",
    "                velocity_stream.append(tr_ehe)\n",
    "                velocity_stream.append(tr_ehn)\n",
    "\n",
    "    # Si tenemos datos en los streams de aceleración y velocidad, los guardamos en archivos separados\n",
    "    if len(acceleration_stream) > 0:\n",
    "        # Guardar el archivo de aceleración\n",
    "        acceleration_stream = st.__class__(acceleration_stream)  # Convertir lista en Stream\n",
    "        acceleration_file = os.path.join(output_folder, f\"UIS05_acceleration_{os.path.basename(file_path)}\")\n",
    "        acceleration_stream.write(acceleration_file, format='MSEED')\n",
    "        print(f\"Archivo de aceleración guardado: {acceleration_file}\")\n",
    "    \n",
    "    if len(velocity_stream) > 0:\n",
    "        # Guardar el archivo de velocidad\n",
    "        velocity_stream = st.__class__(velocity_stream)  # Convertir lista en Stream\n",
    "        velocity_file = os.path.join(output_folder, f\"UIS05_velocity_{os.path.basename(file_path)}\")\n",
    "        velocity_stream.write(velocity_file, format='MSEED')\n",
    "        print(f\"Archivo de velocidad guardado: {velocity_file}\")\n",
    "\n",
    "# Iterar sobre todos los archivos mseed en la carpeta de descargas\n",
    "for filename in os.listdir(download_folder):\n",
    "    if filename.endswith('.mseed'):\n",
    "        file_path = os.path.join(download_folder, filename)\n",
    "        process_mseed(file_path)\n",
    "\n",
    "print(\"Procesamiento completado.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fd0fa",
   "metadata": {},
   "source": [
    "Paso 3: eliminacion de mseed con menos de 6000 muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read\n",
    "import os\n",
    "\n",
    "# Carpeta que contiene los archivos mseed\n",
    "input_folder = 'B_REDNE_mseed_UIS05virtual'\n",
    "valid_folder = 'C_Valid_mseed_files'  # Carpeta para los archivos válidos\n",
    "\n",
    "# Crear la carpeta para archivos válidos si no existe\n",
    "os.makedirs(valid_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos mseed en la carpeta\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.mseed'):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        try:\n",
    "            # Leer el archivo mseed\n",
    "            st = read(file_path)\n",
    "\n",
    "            # Verificar si algún canal tiene menos de 6000 muestras\n",
    "            discard_file = False\n",
    "            for tr in st:\n",
    "                if len(tr.data) < 6000:\n",
    "                    print(f\"Descartando archivo {filename}: el canal {tr.stats.channel} tiene menos de 6000 muestras.\")\n",
    "                    discard_file = True\n",
    "                    break  # Si encontramos un canal que no cumple, descartamos el archivo\n",
    "\n",
    "            # Si el archivo no se descarta, moverlo a la carpeta de archivos válidos\n",
    "            if not discard_file:\n",
    "                print(f\"Archivo {filename} aprobado, tiene canales con al menos 6000 muestras.\")\n",
    "                valid_file_path = os.path.join(valid_folder, filename)\n",
    "                os.rename(file_path, valid_file_path)\n",
    "                print(f\"Archivo {filename} movido a la carpeta de archivos válidos.\")\n",
    "            else:\n",
    "                # Si el archivo no cumple, lo eliminamos directamente\n",
    "                os.remove(file_path)\n",
    "                print(f\"Archivo {filename} eliminado debido a que no cumple con el requisito de 6000 muestras.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf141336",
   "metadata": {},
   "source": [
    "Paso 4: Eliminar tendencia y media según STEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb83721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read\n",
    "import os\n",
    "\n",
    "# Definir la carpeta donde están los archivos mseed\n",
    "download_folder = 'C_Valid_mseed_files'\n",
    "output_folder = 'D_REDNE_mseed_tendencia_media'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos mseed en la carpeta de descargas\n",
    "for filename in os.listdir(download_folder):\n",
    "    if filename.endswith('.mseed'):\n",
    "        file_path = os.path.join(download_folder, filename)\n",
    "\n",
    "        # Leer el archivo mseed\n",
    "        st = read(file_path)\n",
    "        \n",
    "        # Iterar sobre todas las trazas (canales) en el archivo\n",
    "        for tr in st:\n",
    "            # Eliminar la tendencia (ajuste de una línea recta)\n",
    "            tr.detrend(type='linear')  # Ajuste de tendencia lineal\n",
    "            print(f\"Tendencia eliminada en canal {tr.stats.channel} del archivo {filename}\")\n",
    "\n",
    "            # Eliminar la media\n",
    "            tr.detrend(type='constant')  # Eliminar la media\n",
    "            print(f\"Media eliminada en canal {tr.stats.channel} del archivo {filename}\")\n",
    "\n",
    "        # Guardar el archivo procesado\n",
    "        processed_filename = os.path.join(output_folder, f\"processed_{filename}\")\n",
    "        st.write(processed_filename, format='MSEED')\n",
    "        print(f\"Archivo procesado guardado: {processed_filename}\")\n",
    "\n",
    "print(\"Procesamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb435a1",
   "metadata": {},
   "source": [
    "Adaptado y aplicando STA/LTA a los datos mseed para realizar el recorte de la traza y que inicien al mismo tiempo los canales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17302812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Procesamiento en lote de detección de eventos STA/LTA para archivos MSEED.\n",
    "\n",
    "-------\n",
    "* AL leer cada archivo, si el MSEED termina con exactamente 2 eventos,\n",
    "  se recorta la señal completa (todas las trazas del archivo) a una ventana de\n",
    "  1 minuto que comienza 5s antes del primer evento detectado.\n",
    "* Los archivos recortados se guardan en la carpeta `OUTPUT_DIR = mseed_dos_eventos_recortados`\n",
    "\n",
    "FLUJO\n",
    "--------------\n",
    "1. Analiza solo la primera traza para la detección inicial de eventos.\n",
    "2. Aplica STA/LTA con `TH_ON = 3.5` y reprocesa con umbrales 2.7 ó 6.0 según el número de eventos (igual que antes).\n",
    "3. Cuenta archivos con 0, 1, 2, 3 y ≥4 eventos y lista nombres de los grupos 0, 3 y ≥4.\n",
    "4. Recorta los archivos con 2 eventos y los guarda aparte.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta_py, trigger_onset\n",
    "\n",
    "# --------------------------- PARÁMETROS ---------------------------------- #\n",
    "DATA_DIR = \"D_REDNE_mseed_tendencia_media\"      # Carpeta con los .mseed\n",
    "OUTPUT_DIR = \"E_mseed_eventos_umbrales_finales\"      # Carpeta de salida recortes\n",
    "STA_SECONDS = 1\n",
    "LTA_SECONDS = 30        \n",
    "TH_OFF = 1.6            \n",
    "TH_ON_DEFAULT = 4       \n",
    "TH_ON_LOW = 3        \n",
    "TH_ON_HIGH = 6          \n",
    "# ------------------------------------------------------------------------- #\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detectar_eventos(traza, th_on):\n",
    "    \"\"\"Devuelve lista de tiempos UTC (datetime) donde se detectan eventos.\"\"\"\n",
    "    n_sta = int(STA_SECONDS * traza.stats.sampling_rate)\n",
    "    n_lta = int(LTA_SECONDS * traza.stats.sampling_rate)\n",
    "    cft = classic_sta_lta_py(traza.data, n_sta, n_lta)\n",
    "    on_off = trigger_onset(cft, th_on, TH_OFF)\n",
    "    base_time = traza.stats.starttime.datetime\n",
    "    return [base_time + timedelta(seconds=traza.times()[idx_on]) for idx_on, _ in on_off]\n",
    "\n",
    "def procesar_archivo(path, th_on):\n",
    "    \"\"\"Devuelve (canal_id, lista_eventos) usando solo la primera traza.\"\"\"\n",
    "    st = read(path)\n",
    "    tr = st[0]\n",
    "    eventos = detectar_eventos(tr, th_on)\n",
    "    return tr.id, eventos\n",
    "\n",
    "\n",
    "def recortar_y_guardar(path, primer_evento_dt):\n",
    "    \"\"\"Recorta todas las trazas a 60 s iniciando 5 s antes de *primer_evento_dt*.\"\"\"\n",
    "    st = read(path)\n",
    "    start = UTCDateTime(primer_evento_dt) - 5\n",
    "    # Truncar milisegundos y microsegundos dejando solo los segundos exactos\n",
    "    start = UTCDateTime(int(start.timestamp))\n",
    "    end = start + 60\n",
    "    st_rec = st.copy().trim(starttime=start, endtime=end, pad=True, fill_value=0)\n",
    "    out_path = os.path.join(OUTPUT_DIR, os.path.basename(path))\n",
    "    st_rec.write(out_path, format=\"MSEED\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    archivos = sorted(f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".mseed\"))\n",
    "\n",
    "    archivos_0 = []\n",
    "    archivos_1 = []\n",
    "    archivos_2 = [] \n",
    "    archivos_3 = []\n",
    "    archivos_4mas = []\n",
    "\n",
    "    for fname in archivos:\n",
    "        path = os.path.join(DATA_DIR, fname)\n",
    "\n",
    "        try:\n",
    "            canal_id, eventos = procesar_archivo(path, TH_ON_DEFAULT)\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] {fname}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        total_inicial = len(eventos)\n",
    "\n",
    "        # Reprocesar según la cantidad inicial de eventos\n",
    "        if total_inicial == 1:\n",
    "            _, eventos = procesar_archivo(path, TH_ON_LOW)\n",
    "\n",
    "        elif total_inicial >= 3:\n",
    "            _, eventos = procesar_archivo(path, TH_ON_HIGH)\n",
    "\n",
    "        total_final = len(eventos)\n",
    "\n",
    "        # Actualizar listas y contadores\n",
    "        if total_final == 0:\n",
    "            archivos_0.append(fname)\n",
    "        elif total_final == 1:\n",
    "            archivos_1.append(fname)\n",
    "        elif total_final == 2:\n",
    "            archivos_2.append((fname, eventos)) # se guardan los eventos y se recorta\n",
    "        elif total_final == 3:\n",
    "            archivos_3.append(fname)\n",
    "        else:  # ≥4\n",
    "            archivos_4mas.append(fname)\n",
    "\n",
    "    # Se recortan archivos con 2 eventos\n",
    "    for fname, eventos in archivos_2:\n",
    "        path = os.path.join(DATA_DIR, fname)\n",
    "\n",
    "        # Ordenar eventos por seguridad\n",
    "        eventos_ordenados = sorted(eventos)\n",
    "        primer_evt = eventos_ordenados[0]\n",
    "        segundo_evt = eventos_ordenados[1]\n",
    "\n",
    "        # Calcular inicio del recorte\n",
    "        start = UTCDateTime(primer_evt) - 5\n",
    "        start = UTCDateTime(int(start.timestamp))  # eliminar ms\n",
    "\n",
    "        # Condición: segundo evento < 30s después del inicio del recorte\n",
    "        if UTCDateTime(segundo_evt) <= start + 30:\n",
    "            recortar_y_guardar(path, primer_evt)\n",
    "        else:\n",
    "            print(f\"[IGNORADO] {fname}: segundo evento muy tarde\")\n",
    "\n",
    "\n",
    "    # Resumen global\n",
    "    print(\"\\n============= RESUMEN GLOBAL =============\")\n",
    "    print(f\"Total de archivos analizados : {len(archivos)}\")\n",
    "    print(f\"Archivos con 0 eventos       : {len(archivos_0)}\")\n",
    "    print(f\"Archivos con 1 evento        : {len(archivos_1)}\")\n",
    "    print(f\"Archivos con 2 eventos       : {len(archivos_2)}\")\n",
    "    print(f\"Archivos con 3 eventos       : {len(archivos_3)}\")\n",
    "    print(f\"Archivos con ≥4 eventos      : {len(archivos_4mas)}\")\n",
    "\n",
    "    print(\"\\n→ Archivos con 0 eventos:\")\n",
    "    for f in archivos_0:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "    print(\"\\n→ Archivos con 1 eventos:\")\n",
    "    for f in archivos_1:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "    print(\"\\n→ Archivos con 3 eventos:\")\n",
    "    for f in archivos_3:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "    print(\"\\n→ Archivos con 4 o más eventos:\")\n",
    "    for f in archivos_4mas:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbc434",
   "metadata": {},
   "source": [
    "Alineacion de tiempos y cambio de nombre para su trazabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10be84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Organiza los archivos MSEED recortados (1 min, 3 canales) en una sola carpeta.\n",
    "\n",
    "- Lee todos los .mseed que encuentre en la carpeta `E_mseed_eventos_umbrales_finales`.\n",
    "- Guarda cada traza directamente dentro de <OUTPUT_BASE>.\n",
    "- Nomenclatura de salida:\n",
    "    <NETWORK>.<STATION>..<CHANNEL>__YYYYMMDDThhmmssZ__YYYYMMDDThhmmssZ.mseed\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from obspy import read\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# --------------------------- PARÁMETROS ---------------------------------- #\n",
    "INPUT_DIR = \"E_mseed_eventos_umbrales_finales\"  # Carpeta con recortes de 1 min\n",
    "OUTPUT_BASE = \"F_mseed_eventos_finales\"      # Carpeta raíz de salida\n",
    "TIME_FMT = \"%Y%m%dT%H%M%SZ\"                     # Formato de fecha\n",
    "# ------------------------------------------------------------------------- #\n",
    "\n",
    "Path(OUTPUT_BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for fname in sorted(p for p in os.listdir(INPUT_DIR) if p.lower().endswith(\".mseed\")):\n",
    "        path = os.path.join(INPUT_DIR, fname)\n",
    "        try:\n",
    "            st = read(path)\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] {fname}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for tr in st:\n",
    "            net = tr.stats.network or \"UX\"\n",
    "            sta = tr.stats.station\n",
    "            cha = tr.stats.channel\n",
    "            t0: UTCDateTime = tr.stats.starttime\n",
    "            t1: UTCDateTime = tr.stats.endtime\n",
    "            start_str = t0.strftime(TIME_FMT)\n",
    "            end_str = t1.strftime(TIME_FMT)\n",
    "\n",
    "            # Guardar directamente en la carpeta de salida sin subcarpetas\n",
    "            out_fname = f\"{net}.{sta}..{cha}__{start_str}__{end_str}.mseed\"\n",
    "            out_path = Path(OUTPUT_BASE) / out_fname\n",
    "\n",
    "            try:\n",
    "                tr.write(str(out_path), format=\"MSEED\")\n",
    "                print(f\"[OK] {out_path}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[ERROR] No se pudo escribir {out_path}: {exc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38268b74",
   "metadata": {},
   "source": [
    "Estandarización del nombre de los mseed NETWORK.STATION..INSTRUMENT__IDENTIFICADOR.mseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701084b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Une las trazas N, E y Z de cada estación y evento sísmico en un solo archivo MSEED.\n",
    "\n",
    "- Busca todos los .mseed en INPUT_DIR.\n",
    "- Agrupa por estación, instrumento (primeras dos letras del canal) y por evento.\n",
    "- Crea un archivo multicanal por estación, instrumento y evento:\n",
    "    <NETWORK>.<STATION>..<INSTRUMENT>__<IDENTIFICADOR>.mseed\n",
    "- Guarda los archivos combinados en OUTPUT_DIR.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from obspy import read, Stream\n",
    "\n",
    "# -------------------- PARÁMETROS -------------------- #\n",
    "INPUT_DIR = \"F_mseed_eventos_finales\"            # Carpeta con las trazas individuales\n",
    "OUTPUT_DIR = \"G_agrupados_por_evento\"      # Carpeta destino\n",
    "# ---------------------------------------------------- #\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Expresión regular para extraer partes del nombre\n",
    "pattern = re.compile(\n",
    "    r\"^(?P<net>[^.]+)\\.(?P<sta>[^.]+)\\.\\.(?P<cha>[A-Z0-9]+)__\"\n",
    "    r\"(?P<start>\\d{8}T\\d{6}Z)__(?P<end>\\d{8}T\\d{6}Z)\"\n",
    ")\n",
    "\n",
    "# Diccionario para agrupar: {(station, instrument, event_id): [archivos]}\n",
    "groups = {}\n",
    "\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not fname.lower().endswith(\".mseed\"):\n",
    "        continue\n",
    "\n",
    "    match = pattern.match(fname)\n",
    "    if not match:\n",
    "        print(f\"[WARN] No se pudo interpretar el nombre: {fname}\")\n",
    "        continue\n",
    "\n",
    "    net = match.group(\"net\")\n",
    "    sta = match.group(\"sta\")\n",
    "    cha = match.group(\"cha\")\n",
    "    instrument = cha[:2]  # Ejemplo: HNE -> HN, EHZ -> EH, etc.\n",
    "    event_id = f\"{match.group('start')}__{match.group('end')}\"\n",
    "    key = (net, sta, instrument, event_id)\n",
    "\n",
    "    groups.setdefault(key, []).append(fname)\n",
    "\n",
    "# Procesar cada grupo (estación + instrumento + evento)\n",
    "for (net, sta, instrument, event_id), files in groups.items():\n",
    "    st_total = Stream()\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            tr = read(os.path.join(INPUT_DIR, f))\n",
    "            st_total += tr\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] No se pudo leer {f}: {exc}\")\n",
    "\n",
    "    if len(st_total) == 0:\n",
    "        continue\n",
    "\n",
    "    out_name = f\"{net}.{sta}..{instrument}__{event_id}.mseed\"\n",
    "    out_path = Path(OUTPUT_DIR) / out_name\n",
    "\n",
    "    try:\n",
    "        st_total.write(str(out_path), format=\"MSEED\")\n",
    "        print(f\"[OK] {out_name} con {len(st_total)} canales\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[ERROR] No se pudo escribir {out_name}: {exc}\")\n",
    "\n",
    "print(\"\\n Agrupación completada por estación, instrumento y evento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004901b",
   "metadata": {},
   "source": [
    "ULTIMO RECORTE PARA 6000 MUESTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12745adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read, Stream\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ===== CONFIGURACIÓN =====\n",
    "CARPETA_ENTRADA = \"G_agrupados_por_evento\"   # Cambia por tu carpeta real\n",
    "CARPETA_SALIDA  = \"H_ajustados_6000\"       # Cambia por tu carpeta de salida\n",
    "SAMPLES_OBJETIVO = 6000\n",
    "# =========================\n",
    "\n",
    "os.makedirs(CARPETA_SALIDA, exist_ok=True)\n",
    "\n",
    "def ajustar_y_guardar(mseed_path):\n",
    "    st = read(mseed_path)\n",
    "\n",
    "    if len(st) != 3:\n",
    "        print(f\" {os.path.basename(mseed_path)} tiene {len(st)} trazas, se omite.\")\n",
    "        return\n",
    "\n",
    "    st_ajustado = Stream()\n",
    "\n",
    "    for tr in st:\n",
    "        data = tr.data\n",
    "        n = len(data)\n",
    "\n",
    "        if n > SAMPLES_OBJETIVO:\n",
    "            data = data[:SAMPLES_OBJETIVO]  # <- corta del FINAL\n",
    "        elif n < SAMPLES_OBJETIVO:\n",
    "            data = np.pad(data, (0, SAMPLES_OBJETIVO - n), 'constant')  # <- rellena al FINAL\n",
    "\n",
    "        tr.data = data\n",
    "        st_ajustado.append(tr)\n",
    "\n",
    "    ruta_salida = os.path.join(CARPETA_SALIDA, os.path.basename(mseed_path))\n",
    "    st_ajustado.write(ruta_salida, format=\"MSEED\")\n",
    "\n",
    "    print(f\"{os.path.basename(mseed_path)} guardado en salida con 6000 samples por traza.\")\n",
    "\n",
    "for archivo in glob.glob(os.path.join(CARPETA_ENTRADA, \"*.mseed\")):\n",
    "    ajustar_y_guardar(archivo)\n",
    "\n",
    "print(\"\\n Proceso terminado, Todos los archivos en salida tienen 3 canales de 6000 samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06e39c",
   "metadata": {},
   "source": [
    "CREACION DEL HDF5 Y METADATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "# ======== CONFIGURACIÓN ========\n",
    "input_folder = r\"H_ajustados_6000\"      # Carpeta con tus archivos .mseed\n",
    "output_file = r\"../data/REDNE_plantilla.hdf5\"        # Archivo de salida HDF5\n",
    "json_stations = r\"../data/estacionesREDNE_data.json\"  # Archivo JSON con metadatos de estaciones\n",
    "\n",
    "# ======== CARGAR JSON DE ESTACIONES ========\n",
    "with open(json_stations, \"r\", encoding=\"utf-8\") as f:\n",
    "    estaciones_data = json.load(f)\n",
    "\n",
    "# ======== FUNCIÓN PRINCIPAL ========\n",
    "def convert_mseed_to_hdf5(input_folder, output_file):\n",
    "    with h5py.File(output_file, \"a\") as hdf:\n",
    "        data_group = hdf.require_group(\"data\")\n",
    "\n",
    "        # Recorrer archivos nuevos de la carpeta\n",
    "        for file_name in os.listdir(input_folder):\n",
    "            if not file_name.endswith(\".mseed\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            st = read(file_path)\n",
    "\n",
    "            traces = [tr.data.astype(np.float32) for tr in st]\n",
    "            min_len = min(len(tr) for tr in traces)\n",
    "            data_matrix = np.vstack([tr[:min_len] for tr in traces]).T\n",
    "\n",
    "            tr0 = st[0]\n",
    "            station = tr0.stats.station\n",
    "            network = tr0.stats.network\n",
    "            channel_full = tr0.stats.channel\n",
    "            channel_base = channel_full[:-1] if len(channel_full) > 2 else channel_full\n",
    "\n",
    "            starttime = tr0.stats.starttime\n",
    "            start_str = starttime.strftime(\"%Y%m%d%H%M%S\")\n",
    "            trace_name = f\"{station}.{network}_{start_str}_EV\"\n",
    "\n",
    "            # Cargar metadatos desde JSON\n",
    "            est_info = estaciones_data.get(station, {})\n",
    "            coords = est_info.get(\"coords\", [None, None, None])\n",
    "            lat, lon, elev = coords if len(coords) == 3 else (None, None, None)\n",
    "            network_from_json = est_info.get(\"network\", network)\n",
    "\n",
    "\n",
    "            utctime = UTCDateTime(\"2024-05-28T02:54:58.660000Z\")\n",
    "            formatted_time = utctime.datetime.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "            basic_info = {\n",
    "                \"trace_name\": trace_name,\n",
    "                \"trace_start_time\": formatted_time,\n",
    "                \"trace_channel\": channel_base,\n",
    "                \"trace_category\": \"earthquake_local\",\n",
    "                \"station_code\": station,\n",
    "                \"station_network_code\": network_from_json,\n",
    "                \"station_latitude_deg\": lat,\n",
    "                \"station_longitude_deg\": lon,\n",
    "                \"station_elevation_m\": elev,\n",
    "            }\n",
    "\n",
    "            # ======== NO SE BORRAR SI YA EXISTE ========\n",
    "            if trace_name in data_group:\n",
    "                continue\n",
    "            else:\n",
    "                # Crear dataset solo si es nuevo\n",
    "                dset = data_group.create_dataset(\n",
    "                    trace_name,\n",
    "                    data=data_matrix,\n",
    "                    dtype=np.float32,\n",
    "                    compression=\"gzip\"\n",
    "                )\n",
    "\n",
    "                # Guardar SOLO los metadatos que realmente existen\n",
    "                for key, value in basic_info.items():\n",
    "                    if value is None:\n",
    "                        continue  # No guardar nada si está vacío\n",
    "\n",
    "                    if isinstance(value, str):\n",
    "                        dset.attrs[key] = value\n",
    "                    else:\n",
    "                        dset.attrs[key] = np.float32(value)\n",
    "\n",
    "\n",
    "    print(\"\\n Proceso terminado\\n\")\n",
    "\n",
    "# ======== EJECUTAR ========\n",
    "convert_mseed_to_hdf5(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f238c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# folders = [\n",
    "#   r\"E:/TG/BADASI/A_REDNE_mseed_downloads\",\n",
    "#   r\"E:/TG/BADASI/B_REDNE_mseed_UIS05virtual\",\n",
    "#   r\"E:/TG/BADASI/C_Valid_mseed_files\",\n",
    "#   r\"E:/TG/BADASI/D_REDNE_mseed_tendencia_media\",\n",
    "#   r\"E:/TG/BADASI/E_mseed_eventos_umbrales_finales\",\n",
    "#   r\"E:/TG/BADASI/F_mseed_eventos_finales\"\n",
    "#   r\"E:/TG/BADASI/G_agrupados_por_evento\" \n",
    "#   r\"E:/TG/BADASI/H_ajustados_6000\" \n",
    "# ]\n",
    "\n",
    "# for folder in folders:\n",
    "#     if os.path.exists(folder):\n",
    "#         for item in os.listdir(folder):\n",
    "#             item_path = os.path.join(folder, item)\n",
    "\n",
    "#             if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "#                 os.remove(item_path)  # elimina archivos y enlaces\n",
    "#             elif os.path.isdir(item_path):\n",
    "#                 shutil.rmtree(item_path)  # elimina subcarpetas\n",
    "\n",
    "#         print(f\" Contenido eliminado en: {folder}\")\n",
    "\n",
    "# print(\"\\n Limpieza de los directorios completada\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
